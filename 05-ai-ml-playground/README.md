# ğŸ”´ AI/ML Playground

> Where I test 20+ LLM models on my HP Victus (and it gets HOT! ğŸ”¥)

## This is the Cutting Edge Section!

Local LLM deployment, model testing, fine-tuning attempts, and understanding how these beasts actually work. This is where the future happens on my laptop.

## What's Here

- **llm-experiments/** - Testing 20+ different models (Hermes, Dolphin, Wizard, Gemma, Qwen, DeepSeek R1, Claude, Llama, Mistral, etc.)
- **fine-tuning-attempts/** - Custom model training and prompt engineering
- **deployment-guides/** - How to actually run LLMs locally without destroying your GPU

## The Real Talk

ğŸ’¡ Ollama is a lifesaver for local testing
ğŸ’¡ Not all models are created equal for coding tasks
ğŸ’¡ Context matters HUGELY
ğŸ’¡ 6GB VRAM is the limit - optimization is key
ğŸ’¡ Prompt engineering >>> brute force

---

Feel free to use my findings but test for yourself! Every setup is different! ğŸš€
